{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e82a8a4",
   "metadata": {},
   "source": [
    "Stop words are common words which are of little vlue in the text analysis because they occur frequently in the language and typically do not carry much meaning on their own.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da461521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:  The quick brown fox is jumping over the lazy dog. It was a sunny day.\n",
      "Meaningful sentence ['The quick brown fox', 'jumping over', 'lazy dog. It', 'a sunny day.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# sample text \n",
    "text = \"The quick brown fox is jumping over the lazy dog. It was a sunny day.\"\n",
    "\n",
    "# Define stopwords to use a delimeters\n",
    "stopwords_as_delimiters = ['is', 'the', 'was']\n",
    "\n",
    "#create a regex pattern to match stopwords as whole words\n",
    "pattern = r'\\b(?:' + '|'.join(stopwords_as_delimiters) +r')\\b'\n",
    "\n",
    "#Split the text using sopwords as delimiters\n",
    "tokens = re.split(pattern,text)\n",
    "\n",
    "#remove extra whitespace and empty tokens\n",
    "meaningful_phrases = [token.strip() for token in tokens if token.strip()]\n",
    "\n",
    "#display the result\n",
    "print('original text: ', text)\n",
    "print('Meaningful sentence', meaningful_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2052a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# In NLP corpus is a collection of large text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b8b70d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original words ['Stop', 'words', 'are', 'common', 'words', 'that', 'are', 'often', 'considered', 'to', 'be', 'of', 'little', 'value', 'in', 'text', 'analysis']\n",
      "filterd words ['Stop', 'words', 'common', 'words', 'often', 'considered', 'little', 'value', 'text', 'analysis']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Download the NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Sample sentence\n",
    "sentence = \"Stop words are common words that are often considered to be of little value in text analysis\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Get the list of english stop words from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Remove stop words from the tokenized words\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print('original words', words)\n",
    "print('filterd words', filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd26467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90fe718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "812c10f5",
   "metadata": {},
   "source": [
    "### Ques1. \"The sun was shining brightly in the sky, and a gentle breeze was blowing through the trees.\" Write a python program to remove all stop words (e.g \"a\", \"the\", \"was\",\"and\") from the text and return the cleaned version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc4aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "341f1078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text ['The', 'sun', 'was', 'shining', 'brightly', 'in', 'the', 'sky', ',', 'and', 'a', 'gentle', 'breeze', 'was', 'blowing', 'through', 'the', 'trees', '.']\n",
      "Cleaned sentence: sun shining brightly sky , gentle breeze blowing trees .\n"
     ]
    }
   ],
   "source": [
    "# Sample sentence\n",
    "sentence = \"The sun was shining brightly in the sky, and a gentle breeze was blowing through the trees.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Join words to form the cleaned sentence\n",
    "cleaned_sentence = ' '.join(filtered_words)\n",
    "print('original text', words)\n",
    "print(\"Cleaned sentence:\", cleaned_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375d470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
