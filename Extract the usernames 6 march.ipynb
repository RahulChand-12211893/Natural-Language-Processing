{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf9a63d",
   "metadata": {},
   "source": [
    "### Extracting the usernames from the email address present in the text\n",
    "- Using Regex and NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b6c5c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ff5e2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Email Addresses: ['john.doe@example.com', 'jane_smith123@company.org', 'test.email@domain.net', 'help@service.com']\n",
      "Extracted Usernames: ['john.doe', 'jane_smith123', 'test.email', 'help']\n"
     ]
    }
   ],
   "source": [
    "#Download necessary nltk resources\n",
    "nltk.download('punkt')\n",
    "# Sample text containing emails\n",
    "text = \"\"\"\n",
    "Hello, you can reach me at john.doe@example.com and my colleague at jane_smith123@company.org.\n",
    "Another email is test.email@domain.net, and support can be contacted at help@service.com.\n",
    "\"\"\"\n",
    "\n",
    "# Corrected regex pattern to match email addresses\n",
    "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "\n",
    "# Extract emails from the text using re.findall()\n",
    "emails = re.findall(email_pattern, text)\n",
    "\n",
    "# Extract usernames (part before '@')\n",
    "usernames = [email.split('@')[0] for email in emails]\n",
    "\n",
    "# Display the results\n",
    "print(\"Extracted Email Addresses:\", emails)\n",
    "print(\"Extracted Usernames:\", usernames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47750106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2bcf0ab",
   "metadata": {},
   "source": [
    "### Extracting Top common words- Extract the top 10 most common words in the text excluding stopwords.\n",
    "- You can extract the top 10 most common words in a given text excluding stopwords using NLTK as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14d9b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b951c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most coomon words (Exclusting stopwords): \n",
      "natural:2\n",
      "language:2\n",
      "nlp:2\n",
      "machine:2\n",
      "processing:1\n",
      "subfield:1\n",
      "artificial:1\n",
      "intelligence:1\n",
      "focuses:1\n",
      "interaction:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading stpwords: Package 'stpwords' not found in\n",
      "[nltk_data]     index\n"
     ]
    }
   ],
   "source": [
    "#Download necessary dataset\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# sample text\n",
    "text = \"\"\"\n",
    "Natural Language processing (NLP) is a subfield of artificial intelligence that focuses on the interaction\n",
    "between computers and humansusing natural language. It  involves the development of algorithm and models\n",
    "that enable machine to understand, interpret, and generate human-like text. NLP plays a crucial role in \n",
    "various applications, including sentimentanalysis, machine translation, chatbots, and information extraction.\n",
    "\"\"\"\n",
    "\n",
    "#Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "#convert all words to lowercase for uniformity\n",
    "tokens = [word.lower() for word in tokens]\n",
    "\n",
    "#Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Remove stopwords and punctuations \n",
    "filtered_words = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "#COunt word frequencies\n",
    "word_counts = Counter(filtered_words)\n",
    "\n",
    "#Finding the 10 most common words\n",
    "top_10_common = word_counts.most_common(10)\n",
    "\n",
    "\n",
    "#Display the top 10 common words\n",
    "print(\"Top 10 most coomon words (Exclusting stopwords): \")\n",
    "for word, count in top_10_common:\n",
    "    print(f'{word}:{count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3419046b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
